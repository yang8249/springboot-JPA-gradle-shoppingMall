package com.yang.shopping.service;

import co.elastic.clients.elasticsearch._types.FieldValue;
import co.elastic.clients.elasticsearch._types.SortOrder;
import co.elastic.clients.elasticsearch._types.aggregations.Aggregate;
import co.elastic.clients.elasticsearch._types.aggregations.Aggregation;
import co.elastic.clients.elasticsearch._types.aggregations.StringTermsBucket;
import co.elastic.clients.elasticsearch._types.query_dsl.BoolQuery;
import co.elastic.clients.elasticsearch._types.query_dsl.Query;
import co.elastic.clients.elasticsearch.core.SearchRequest;
import co.elastic.clients.elasticsearch.core.SearchResponse;
import co.elastic.clients.elasticsearch.transform.Settings;
import co.elastic.clients.json.JsonpMapper;
import co.elastic.clients.json.JsonpUtils;
import co.elastic.clients.json.jackson.JacksonJsonpMapper;
import jakarta.annotation.PostConstruct;
import jakarta.json.spi.JsonProvider;
import jakarta.json.stream.JsonGenerator;
import lombok.RequiredArgsConstructor;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.data.domain.Sort;
import org.springframework.data.elasticsearch.client.elc.NativeQuery;
import org.springframework.data.elasticsearch.client.elc.NativeQueryBuilder;
import org.springframework.data.elasticsearch.client.elc.ElasticsearchAggregation;
import org.springframework.data.elasticsearch.client.elc.ElasticsearchAggregations;
import org.springframework.data.elasticsearch.core.AggregationsContainer;
import org.springframework.data.elasticsearch.core.ElasticsearchOperations;
import org.springframework.data.elasticsearch.core.IndexOperations;
import org.springframework.data.elasticsearch.core.SearchHit;
import org.springframework.data.elasticsearch.core.SearchHits;
import org.springframework.data.elasticsearch.core.query.HighlightQuery;
import org.springframework.data.elasticsearch.core.query.highlight.Highlight;
import org.springframework.data.elasticsearch.core.query.highlight.HighlightField;
import org.springframework.data.elasticsearch.core.query.highlight.HighlightFieldParameters;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.util.CollectionUtils;
import org.springframework.util.StringUtils;

import com.yang.shopping.dto.ArticleInsightsResponse;
import com.yang.shopping.dto.ArticleRequest;
import com.yang.shopping.dto.ArticleResponse;
import com.yang.shopping.dto.ArticleSearchHitResponse;
import com.yang.shopping.dto.ArticleSearchPageResponse;
import com.yang.shopping.dto.BulkLoadResponse;
import com.yang.shopping.dto.ReindexResponse;
import com.yang.shopping.dto.SuggestionResponse;
import com.yang.shopping.mapper.ArticleMapper;
import com.yang.shopping.model.Article;
import com.yang.shopping.model.Product;
import com.yang.shopping.model.search.ArticleDocument;
import com.yang.shopping.repository.ArticleRepository;
import com.yang.shopping.repository.ProductRepository;

import java.io.StringWriter;
import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneOffset;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.Set;
import java.util.concurrent.ThreadLocalRandom;
import java.util.concurrent.atomic.AtomicReference;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
public class ArticleService {

    private static final String CATEGORY_AGG = "by_category";
    private static final String TAGS_AGG = "by_tags";
    private static final String AVG_RATING_AGG = "avg_rating";

    private static final String CATEGORY_KEYWORD_FIELD = "category.keyword";
    private static final String TAGS_KEYWORD_FIELD = "tags.keyword";
    private static final String RATING_FIELD = "rating";
    private static final String PUBLISHED_AT_FIELD = "publishedAt";

    private final ArticleRepository repository;
    private final ElasticsearchOperations elasticOps;
    private final ArticleMapper mapper;

	@Autowired
	private ProductRepository productRepository;
	private ArticleRepository articleRepository;
	
    public ArticleSearchPageResponse searchProducts(String keyword, int page, int size) {

		System.out.println("ì—˜ë¼ìŠ¤í‹± ì„œë¹„ìŠ¤ ì§„ì…ì™„ë£Œ");
		
        Pageable pageable = PageRequest.of(page, size);
        NativeQuery query = NativeQuery.builder()
            .withQuery(q -> q.match(m -> m.field("title").query(keyword)))
            .withPageable(pageable)
            .build();

        SearchHits<ArticleDocument> hits = elasticOps.search(query, ArticleDocument.class);
        List<ArticleResponse> products = hits.getSearchHits().stream()
            .map(hit -> mapper.toResponse(hit.getContent()))
            .toList();

        long total = hits.getTotalHits();
        System.out.println("ê²€ìƒ‰ ì™„ë£Œ: " + total + "ê°œì˜ ìƒí’ˆì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.");
        System.out.println("ê²€ìƒ‰ëœ ìƒí’ˆ: " + products);
        return new ArticleSearchPageResponse(products, total);
    }
    
    public ArticleSearchPageResponse searchProductsSame(String keyword, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);

        NativeQuery query = NativeQuery.builder()
                .withQuery(q -> q
                    .multiMatch(m -> m
                        .fields("title", "content", "category")
                        .query(keyword)
                        .fuzziness("AUTO") // ì˜¤íƒ€ í—ˆìš©
                    )
                )
                .withPageable(pageable)
                .build();



        SearchHits<ArticleDocument> hits = elasticOps.search(query, ArticleDocument.class);

        List<ArticleResponse> products = hits.getSearchHits().stream()
                .map(hit -> mapper.toResponse(hit.getContent()))
                .toList();

        long total = hits.getTotalHits();

        System.out.println("ê²€ìƒ‰ ì™„ë£Œ: " + total + "ê°œì˜ ìƒí’ˆì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.");
        System.out.println("ê²€ìƒ‰ëœ ìƒí’ˆ: " + products);

        return new ArticleSearchPageResponse(products, total);
    }
//    public List<ArticleDocument> autocompleteTitle(String keyword, String category) {
//        NativeQuery query = NativeQuery.builder()
//            .withQuery(q -> q.bool(b -> {
//                b.must(m -> m.matchPhrasePrefix(mp -> mp.field("title").query(keyword).maxExpansions(10)));
//                if (category != null && !category.isEmpty()) {
//                    b.filter(f -> f.term(t -> t.field("category.keyword").value(category)));
//                }
//            }))
//            .withPageable(PageRequest.of(0, 5))
//            .build();
//
//        SearchHits<ArticleDocument> hits = elasticOps.search(query, ArticleDocument.class);
//
//        return hits.getSearchHits().stream()
//                   .map(SearchHit::getContent)
//                   .toList();
//    }
	 
    @PostConstruct
    void ensureIndex() {
        try {
            IndexOperations indexOps = elasticOps.indexOps(ArticleDocument.class);
            System.out.println("Elasticsearch ì—°ê²° ì •ìƒ, ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€: " + indexOps.exists());
            if (!indexOps.exists()) {
                indexOps.create();
                indexOps.putMapping();
                System.out.println("Elasticsearch ì„œë²„ì´ë¯¸ ìˆìŒ: " + indexOps);
                ensureIndexAndIndexData();
            }
        } catch (Exception e) {
            System.out.println("Elasticsearch ì„œë²„ ì—†ìŒ: " + e.getMessage());
        }
    }
    @PostConstruct
    void ensureIndexAndIndexData() {
        try {
            IndexOperations indexOps = elasticOps.indexOps(ArticleDocument.class);

            // 1ï¸âƒ£ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
            if (indexOps.exists()) {
                indexOps.delete();
                System.out.println("ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ");
            }

            // ì„¤ì • Mapìœ¼ë¡œ analyzer ì§€ì •
            Map<String, Object> settings = Map.of(
                "analysis", Map.of(
                    "analyzer", Map.of(
                        "korean_synonym_analyzer", Map.of(
                            "type", "custom",
                            "tokenizer", "nori_tokenizer",
                            "filter", List.of("lowercase", "synonym_filter")
                        )
                    ),
                    "filter", Map.of(
                        "synonym_filter", Map.of(
                            "type", "synonym",
                            "synonyms_path", "analysis/synonyms.txt"
                        )
                    )
                )
            );

            // ì¸ë±ìŠ¤ ìƒì„± + ë§¤í•‘ ì ìš©
            indexOps.create(settings);
            indexOps.putMapping(indexOps.createMapping());
            System.out.println("ìƒˆ ì¸ë±ìŠ¤ ë° ë§¤í•‘ ìƒì„± ì™„ë£Œ");

            // 3ï¸âƒ£ DB ì „ì²´ ì¡°íšŒ í›„ ìƒ‰ì¸
            List<Product> products = productRepository.findAll();
            for (Product p : products) {
                ArticleDocument doc = mapToDocument(p);
                elasticOps.save(doc);
            }

            System.out.println("DB ë°ì´í„°ë¥¼ Elasticsearchì— ìƒˆë¡œ ìƒ‰ì¸ ì™„ë£Œ (ì´ " + products.size() + "ê±´)");

        } catch (Exception e) {
            System.out.println("Elasticsearch ì˜¤ë¥˜: " + e.getMessage());
        }
    }

    private ArticleDocument mapToDocument(Product p) {
        ArticleDocument doc = new ArticleDocument();
        doc.setProductId(String.valueOf(p.getProductSeq()));
        doc.setTitle(p.getProductName());
        doc.setContent(p.getContent());
        
        List<String> colorList = Arrays.stream(p.getColor().split(","))
                .map(String::trim)
                .collect(Collectors.toList());
        doc.setColor(colorList);
        doc.setCategory(p.getCategory());
        return doc;
    }





    @Transactional
    public ArticleResponse save(ArticleRequest request) {
        Article entity = mapper.toEntity(request);
        return persist(entity);
    }

    public List<ArticleResponse> findAll() {
        return repository.findAll().stream()
                .map(mapper::toResponse)
                .toList();
    }

    public ArticleSearchPageResponse searchTitle(String title, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        NativeQuery query = baseQuery(pageable)
                .withQuery(q -> q.match(m -> m.field("title").query(title)))
                .build();
        return execute(query);
    }

    public ArticleSearchPageResponse fullTextSearch(String text, int page, int size) {
        Pageable pageable = PageRequest.of(page, size);
        NativeQuery query = baseQuery(pageable)
                .withQuery(q -> q.multiMatch(m -> m
                        .query(text)
                        .fields("title^2", "content", "tags")))
                .build();
        return execute(query);
    }


    
    @Autowired
    private ElasticsearchOperations elasticsearchOperations;
    public List<ArticleDocument> advancedSearch(
            String text,
            String category, // ì¶”ê°€
            Set<String> tags,
            LocalDateTime publishedFrom,
            LocalDateTime publishedTo,
            Double minRating,
            int page,
            int size
    ) {

        BoolQuery.Builder boolBuilder = new BoolQuery.Builder();

        // ğŸ”¹ 1ï¸âƒ£ í•µì‹¬ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘
        Map<String, String> keywordToCategory = Map.ofEntries(
            Map.entry("ì•„ìš°í„°", "outer"),
            Map.entry("ì½”íŠ¸", "outer"),
            Map.entry("ì¬í‚·", "outer"),
            Map.entry("ë¸”ë ˆì´ì €", "outer"),
            Map.entry("íŒ¨ë”©", "outer"),
            Map.entry("ì í¼", "outer"),
            Map.entry("íŠ¸ë Œì¹˜ì½”íŠ¸", "outer"),

            Map.entry("ìƒì˜", "tops"),
            Map.entry("í‹°ì…”ì¸ ", "tops"),
            Map.entry("ì…”ì¸ ", "tops"),
            Map.entry("ë¸”ë¼ìš°ìŠ¤", "tops"),
            Map.entry("ë‹ˆíŠ¸", "tops"),
            Map.entry("ê°€ë””ê±´", "tops"),
            Map.entry("í›„ë“œ", "tops"),
            Map.entry("ë§¨íˆ¬ë§¨", "tops"),

            Map.entry("ë°”ì§€", "bottoms"),
            Map.entry("ìŠ¬ë™ìŠ¤", "bottoms"),
            Map.entry("ì²­ë°”ì§€", "bottoms"),
            Map.entry("ì¡°ê±°íŒ¬ì¸ ", "bottoms"),
            Map.entry("ë ˆê¹…ìŠ¤", "bottoms"),
            Map.entry("ì¹˜ë§ˆ", "bottoms"),

            Map.entry("ì›í”¼ìŠ¤", "dresses"),
            Map.entry("ë“œë ˆìŠ¤", "dresses"),
            Map.entry("íˆ¬í”¼ìŠ¤", "dresses"),
            Map.entry("ë¡±ì›í”¼ìŠ¤", "dresses"),
            Map.entry("ë¯¸ë‹ˆì›í”¼ìŠ¤", "dresses"),
            Map.entry("ë©ì›í”¼ìŠ¤", "dresses"),

            Map.entry("ê°€ë°©", "accessories"),
            Map.entry("ëª¨ì", "accessories"),
            Map.entry("ë²¨íŠ¸", "accessories")
        );

        // ğŸ”¹ 2ï¸âƒ£ ìƒ‰ìƒ ë¦¬ìŠ¤íŠ¸
        List<String> knownColors = List.of(
        	    "ê²€ì •", "ê²€ì •ìƒ‰", "ë¸”ë™",
        	    "í™”ì´íŠ¸", "í°ìƒ‰", "ì•„ì´ë³´ë¦¬", "í™”ì´íŠ¸ìƒ‰", "í°ìƒ‰ìƒ‰", "ì•„ì´ë³´ë¦¬ìƒ‰",
        	    "ë² ì´ì§€", "ë² ì´ì§€ìƒ‰", "ì¹´í‚¤", "ì¹´í‚¤ìƒ‰", "ë¸Œë¼ìš´", "ë¸Œë¼ìš´ìƒ‰",
        	    "ë ˆë“œ", "ë ˆë“œìƒ‰", "ë¹¨ê°•", "ë¹¨ê°•ìƒ‰", "í•‘í¬", "í•‘í¬ìƒ‰",
        	    "ë¸”ë£¨", "ë¸”ë£¨ìƒ‰", "íŒŒë‘", "íŒŒë‘ìƒ‰", "ë‚¨ìƒ‰", "ë‚¨ìƒ‰ìƒ‰",
        	    "ì—°ë‘", "ì—°ë‘ìƒ‰", "ë¯¼íŠ¸", "ë¯¼íŠ¸ìƒ‰", "ì˜¬ë¦¬ë¸Œ", "ì˜¬ë¦¬ë¸Œìƒ‰",
        	    "ë…¸ë‘", "ë…¸ë‘ìƒ‰", "ë¨¸ìŠ¤íƒ€ë“œ", "ë¨¸ìŠ¤íƒ€ë“œìƒ‰", "ì˜¤ë Œì§€", "ì˜¤ë Œì§€ìƒ‰",
        	    "ë³´ë¼", "ë³´ë¼ìƒ‰", "í¼í”Œ", "í¼í”Œìƒ‰", "ì—°ë³´ë¼", "ì—°ë³´ë¼ìƒ‰",
        	    "ê·¸ë ˆì´", "ê·¸ë ˆì´ìƒ‰", "íšŒìƒ‰", "íšŒìƒ‰ìƒ‰", "ì°¨ì½œ", "ì°¨ì½œìƒ‰", "ì§„íšŒìƒ‰", "ì§„íšŒìƒ‰ìƒ‰"
        	);


        AtomicReference<String> detectedCategory = new AtomicReference<>(null);
        Set<String> detectedColors = new HashSet<>();
        List<String> remainingTokens = new ArrayList<>();

        System.out.println("text " + text);
        if (StringUtils.hasText(text)) {
            String[] tokens = text.split("\\s+");
            for (String token : tokens) {
                System.out.println("token " + token);
                // ì¹´í…Œê³ ë¦¬
                if (detectedCategory.get() == null && keywordToCategory.containsKey(token)) {
                    System.out.println("ì¹´í…Œê³ ë¦¬ ì§„ì… " + token);
                    detectedCategory.set(keywordToCategory.get(token));
                    continue;
                }
                // ìƒ‰ìƒ
                if (knownColors.contains(token)) {
                    System.out.println("ìƒ‰ìƒ ì§„ì… " + token);
                    detectedColors.add(token);
                    continue;
                }
                System.out.println("ìœ ì‚¬ ê²€ìƒ‰ìš© ì§„ì…" + token);
                // ë‚˜ë¨¸ì§€ ìœ ì‚¬ ê²€ìƒ‰ìš©
                remainingTokens.add(token);
            }
        }
        System.out.println("ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: " + detectedCategory.get());
        System.out.println("ê°ì§€ëœ ì»¬ëŸ¬: " + detectedColors);
        System.out.println("ê°ì§€ëœ ìœ ì‚¬í‚¤ì›Œë“œ: " + remainingTokens);
        System.out.println("\n\n===============================\n\n");
        

     // 1ï¸âƒ£ category í•„í„°
        if (detectedCategory.get() != null) {
        	
        	System.out.println("ìµœì¢… ì¹´í…Œê³ ë¦¬ í•„í„° ê°’: " + detectedCategory.get());
            boolBuilder.filter(f -> f.term(t -> t.field("category").value(detectedCategory.get())));
        }

        // 2ï¸âƒ£ color í•„í„° (filterë¡œ ë³€ê²½)
        if (!detectedColors.isEmpty()) {
            Set<String> cleanedColors = detectedColors.stream()
                .map(c -> c.replace("ìƒ‰", ""))
                .collect(Collectors.toSet());
            List<FieldValue> colorValues = cleanedColors.stream()
                .map(FieldValue::of)
                .toList();
            
            System.out.print("ìµœì¢… ìƒ‰ìƒ í•„í„° ê°’:");
            colorValues.forEach(v -> System.out.print(" "+v._get()));


            boolBuilder.filter(f -> f.terms(t -> t.field("color").terms(tv -> tv.value(colorValues))));
        }

        System.out.println("");
        // 3ï¸âƒ£ ìœ ì‚¬ê²€ìƒ‰
        if (!remainingTokens.isEmpty()) {
            String remainingText = String.join(" ", remainingTokens);
            System.out.println("ìµœì¢… ìœ ì‚¬ê²€ìƒ‰ í…ìŠ¤íŠ¸: " + remainingText);
            // title ì •í™•ì¼ì¹˜ boost
            boolBuilder.should(s -> s.match(m -> m.field("title").query(remainingText).boost(10.0f)));

            // title + content fuzziness
            boolBuilder.should(s -> s.multiMatch(mm -> mm
                .query(remainingText)
                .fields("title", "content")
                .fuzziness("AUTO")
            ));

            boolBuilder.minimumShouldMatch("1");
        }

        

       

        // ğŸ”¹ 8ï¸âƒ£ NativeQuery
        NativeQuery query = NativeQuery.builder()
                .withQuery(q -> q.bool(boolBuilder.build()))
                .withMaxResults(size)
                .withSort(so -> so.score(sc -> sc.order(SortOrder.Desc)))
                .build();

        // ğŸ”¹ 9ï¸âƒ£ ì‹¤í–‰
        return elasticsearchOperations.search(query, ArticleDocument.class)
                .stream()
                .map(SearchHit::getContent)
                .toList();
    }

    public SuggestionResponse suggestTitles(String prefix, int size) {
        NativeQuery query = NativeQuery.builder()
                .withQuery(q -> q.matchPhrasePrefix(m -> m.field("title").query(prefix)))
                .withPageable(PageRequest.of(0, size))
                .build();
        SearchHits<ArticleDocument> hits = elasticOps.search(query, ArticleDocument.class);
        List<String> suggestions = hits.getSearchHits().stream()
                .map(hit -> hit.getContent().getTitle())
                .distinct()
                .limit(size)
                .toList();
        return new SuggestionResponse(suggestions);
    }

    public BulkLoadResponse loadSampleData(int count) {
        List<ArticleResponse> created = new ArrayList<>();
        for (int i = 0; i < count; i++) {
            created.add(persist(generateSampleArticle(i)));
        }
        return new BulkLoadResponse(created.size());
    }

    public ArticleInsightsResponse insights() {
        NativeQuery query = baseQuery(Pageable.unpaged())
                .withQuery(q -> q.matchAll(ma -> ma))
                .build();
        SearchHits<ArticleDocument> hits = elasticOps.search(query, ArticleDocument.class);
        return buildInsights(hits.getAggregations());
    }

    public void delete(Long id) {
        repository.deleteById(id);
        elasticOps.delete(String.valueOf(id), ArticleDocument.class);
        elasticOps.indexOps(ArticleDocument.class).refresh();
    }

    @Transactional(readOnly = true)
    public ReindexResponse reindexFromDatabase() {
        List<Article> articles = repository.findAll();
        if (articles.isEmpty()) {
            elasticOps.indexOps(ArticleDocument.class).refresh();
            return new ReindexResponse(0, 0);
        }

        List<ArticleDocument> documents = articles.stream()
                .map(mapper::toDocument)
                .filter(doc -> doc.getId() != null)
                .toList();

        documents.forEach(elasticOps::save);
        elasticOps.indexOps(ArticleDocument.class).refresh();

        return new ReindexResponse(articles.size(), documents.size());
    }

    private ArticleResponse persist(Article entity) {
        if (entity.getPublishedAt() == null) {
            entity.setPublishedAt(LocalDateTime.now());
        }
        if (entity.getTags() == null) {
            entity.setTags(new HashSet<>());
        }
        Article saved = repository.save(entity);
        ArticleDocument document = mapper.toDocument(saved);
        elasticOps.save(document);
        elasticOps.indexOps(ArticleDocument.class).refresh();
        System.out.println("ì €ì¥ëœ ArticleDocument: " + document);

        return mapper.toResponse(saved);
    }

    private NativeQueryBuilder baseQuery(Pageable pageable) {
        HighlightField titleHighlight = new HighlightField("title",
                HighlightFieldParameters.builder()
                        .withPreTags("<em>")
                        .withPostTags("</em>")
                        .build());
        HighlightField contentHighlight = new HighlightField("content",
                HighlightFieldParameters.builder()
                        .withPreTags("<em>")
                        .withPostTags("</em>")
                        .withFragmentSize(180)
                        .build());
        Highlight highlight = new Highlight(List.of(titleHighlight, contentHighlight));

        return NativeQuery.builder()
                .withPageable(pageable)
                .withHighlightQuery(new HighlightQuery(highlight, ArticleDocument.class))
                .withAggregation(CATEGORY_AGG, Aggregation.of(a -> a.terms(t -> t.field(CATEGORY_KEYWORD_FIELD).size(10))))
                .withAggregation(TAGS_AGG, Aggregation.of(a -> a.terms(t -> t.field(TAGS_KEYWORD_FIELD).size(15))))
                .withAggregation(AVG_RATING_AGG, Aggregation.of(a -> a.avg(avg -> avg.field(RATING_FIELD))));
    }

    private ArticleSearchPageResponse execute(NativeQuery query) {
        SearchHits<ArticleDocument> hits = elasticOps.search(query, ArticleDocument.class);
        List<ArticleSearchHitResponse> items = hits.getSearchHits().stream()
                .map(this::mapHit)
                .toList();
        ArticleInsightsResponse insights = buildInsights(hits.getAggregations());
        long total = hits.getTotalHits();
        return new ArticleSearchPageResponse(items, total, insights);
    }

    private ArticleSearchHitResponse mapHit(SearchHit<ArticleDocument> hit) {
        ArticleResponse article = mapper.toResponse(hit.getContent());
        Map<String, List<String>> highlights = hit.getHighlightFields();
        double score = hit.getScore();
        return new ArticleSearchHitResponse(article, score, highlights);
    }

    private ArticleInsightsResponse buildInsights(AggregationsContainer<?> container) {
        if (!(container instanceof ElasticsearchAggregations elasticsearchAggregations)) {
            return new ArticleInsightsResponse(Collections.emptyMap(), Collections.emptyMap(), null);
        }
        Map<String, ElasticsearchAggregation> aggregationMap = elasticsearchAggregations.aggregationsAsMap();
        Map<String, Long> categoryCounts = extractTerms(aggregationMap.get(CATEGORY_AGG));
        Map<String, Long> tagCounts = extractTerms(aggregationMap.get(TAGS_AGG));
        Double averageRating = Optional.ofNullable(aggregationMap.get(AVG_RATING_AGG))
                .map(ElasticsearchAggregation::aggregation)
                .map(org.springframework.data.elasticsearch.client.elc.Aggregation::getAggregate)
                .map(Aggregate::avg)
                .map(avg -> avg.value())
                .orElse(null);
        return new ArticleInsightsResponse(categoryCounts, tagCounts, averageRating);
    }

    private Map<String, Long> extractTerms(ElasticsearchAggregation aggregation) {
        if (aggregation == null) {
            return Collections.emptyMap();
        }
        Aggregate aggregate = aggregation.aggregation().getAggregate();
        if (aggregate == null || aggregate.sterms() == null || aggregate.sterms().buckets() == null) {
            return Collections.emptyMap();
        }
        return aggregate.sterms().buckets().array().stream()
                .collect(Collectors.toMap(
                        bucket -> {
                            FieldValue key = bucket.key();
                            if (key == null) {
                                return "";
                            }
                            if (key.isString()) {
                                return key.stringValue();
                            }
                            return key._toJsonString();
                        },
                        StringTermsBucket::docCount,
                        Long::sum,
                        LinkedHashMap::new));
    }

    private BoolQuery.Builder applyFilters(BoolQuery.Builder builder,
            String text,
            String category,
            Set<String> tags,
            LocalDateTime publishedFrom,
            LocalDateTime publishedTo,
            Double minRating) {

			boolean[] hasMust = {false};
			boolean[] hasFilter = {false};
			
			if (StringUtils.hasText(text)) {
			// ğŸ”¹ multi_match + fuzziness
			builder.should(m -> m.multiMatch(mm -> mm
			.query(text)
			.fields("title^3", "content^2", "tags") // ê°€ì¤‘ì¹˜ ì§€ì •
			.fuzziness("AUTO") // ì˜¤íƒ€ í—ˆìš©
			.prefixLength(1)   // fuzzy ì ìš© ìµœì†Œ ê¸€ì ê¸¸ì´
			));
			
			// ğŸ”¹ wildcard ë³´ì¡° (ë¶€ë¶„ ì¼ì¹˜)
			builder.should(m -> m.wildcard(w -> w
			.field("title")
			.value("*" + text + "*")
			.boost(2.0f)
			));
			builder.should(m -> m.wildcard(w -> w
			.field("content")
			.value("*" + text + "*")
			.boost(1.5f)
			));
			
			builder.minimumShouldMatch("1"); // should ì¤‘ í•˜ë‚˜ë§Œ ë§ì•„ë„ ê²€ìƒ‰
			hasMust[0] = true;
			}
			
			// ì¹´í…Œê³ ë¦¬ í•„í„°
			if (StringUtils.hasText(category)) {
			builder.filter(f -> f.term(t -> t.field("category").value(category)));
			hasFilter[0] = true;
			}
			
			// íƒœê·¸ í•„í„°
			if (!CollectionUtils.isEmpty(tags)) {
			List<FieldValue> values = tags.stream().map(FieldValue::of).toList();
			builder.filter(f -> f.terms(t -> t.field("tags").terms(tv -> tv.value(values))));
			hasFilter[0] = true;
			}
			
			// ë‚ ì§œ í•„í„°
			if (publishedFrom != null || publishedTo != null) {
			builder.filter(f -> f.range(r -> r.date(d -> {
			d.field("publishedAt");
			if (publishedFrom != null)
			d.gte(publishedFrom.atOffset(ZoneOffset.UTC).toInstant().toString());
			if (publishedTo != null)
			d.lte(publishedTo.atOffset(ZoneOffset.UTC).toInstant().toString());
			return d;
			})));
			hasFilter[0] = true;
			}
			
			// ìµœì†Œ í‰ì  í•„í„°
			if (minRating != null) {
			builder.filter(f -> f.range(r -> r.number(n -> n.field("rating").gte(minRating))));
			hasFilter[0] = true;
			}
			
			// ì•„ë¬´ ì¡°ê±´ ì—†ìœ¼ë©´ match_all
			if (!hasMust[0] && !hasFilter[0]) {
			builder.must(m -> m.matchAll(ma -> ma));
			}
			
			return builder;
			}



    private Article generateSampleArticle(int seed) {
        String[] categories = { "Tech", "Business", "Culture", "Sports", "Science" };
        String[] authors = { "Alice Kim", "Brian Lee", "Chris Park", "Dana Cho", "Evan Jung" };
        String[] keywordPool = { "Elasticsearch", "Scalability", "Realtime", "Search", "Analytics", "Dashboard",
                "Indexing", "Observability" };

        ThreadLocalRandom random = ThreadLocalRandom.current();

        Set<String> tags = random.ints(0, keywordPool.length)
                .distinct()
                .limit(3)
                .mapToObj(i -> keywordPool[i])
                .collect(Collectors.toSet());

        LocalDateTime publishedAt = LocalDateTime.now().minusDays(random.nextInt(0, 60));

        return Article.builder()
                .title("Elastic ì²´í—˜ ë…¸íŠ¸ #" + (seed + 1))
                .content("Elastic Stackìœ¼ë¡œ " + tags.iterator().next() + "ë¥¼ êµ¬í˜„í•œ ì‚¬ë¡€ì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
                .author(authors[random.nextInt(authors.length)])
                .category(categories[random.nextInt(categories.length)])
                .rating(Math.round((random.nextDouble(3.5, 5.0)) * 10d) / 10d)
                .publishedAt(publishedAt)
                .tags(tags)
                .build();
    }
}
